To get cluster ip:

minikube ip

Pods are not meant to be seen from outside of the cluster, so they are only accessible from inside the cluster by default.

//////////////////////////////

SERVICES:

Since pods are ephemeral and come/go/live/die/restart, services are there to make sure they are pointed at any instance of a pod, no matter if its restarted or what.... With a service, we CAN connect to the k8s cluster and the service will find whatever pod to fill a request.

Pod Labels:
We can set up a series of key/value pairs (one or more pair). For example, we can give a pod a label whos key is 'app' and value is 'webapp'. You can give any labels you want, it's up to you. When we create a service, we give it a selector (key/value pair), such as app:webapp - and the service will look for any pods that have that key/value pair. So for a service with 

selector:
  app:webapp

The service will look for any pods that have the label
app:webapp

So next we'll create a service to point to our webapp pod!

For the service yaml, the metadata -> name, is absolutely critical - it needs to be a unique name. So you as an architect need to create a system for naming unique services.

If you make your service of type ClusterIP, you tell k8s that this service will only be accessible from within the cluster (good for internal microservices). Many microservices shouldn't be exposed to external traffic, so the type: ClusterIP would be good for those.
If you need your service to be accessible to outside traffic, use type: NodePort

The type NodePort exposes the port through the worker node. We can actually choose which NodePort we want to expose. we can add a key/value pair

ports:
  - name: http
    port: 80 # which pod port
    nodePort: 30080 # must be greater than 30000

Once the service and pod are applied, use minikube ip

it will give you the ip of the cluster, then since we have a NodePort 30080 on the service, we can use a browser to the cluster ip address:30080, and it will give us the app running in the pod!

So let's say that our pod/container/application has been updated - the dev's have added a new function to it. You could simply go into the pod yaml file and change the container image. The problem with this, is that when you applied the file, it would have to kill the pod, pull the new container image, etc. which means downtime.....There are other, built in elegant methods built into k8s which we will get into later......... For now, we will use labels which will be a workaround.

For this, we will add an additional label to the pod yaml,
release:0

Then we add that into the selection rules of the service, telling the service to target any pods with labels
app: webapp AND
release: 0

We will then create a second pod with those labels, which will allow the old pod to still serve, while the new pod comes up. Change the service yaml and then apply the service.


/////////////////////////////////////

Deploying ActiveMQ as a pod/service:

We will deploy a queue service. Let's pretend the system developer decides a queue is needed. My job, is to deploy the queue and ensure it is working.

The image is richardchesterwood/k8s-fleetman-queue:release-1

Port 8161 is the admin console with admin/admin

Create a pod with the image (dont worry about port), then a service with nodePort 30010 and pod port 8161

///////////////////////////////////

ReplicaSets:

Pods will die and restart, we know this. If the node dies were in trouble, but pods are expected to come and go. They are super basic. If a pod consumes too many resources like CPU/Memory, the node will execute it. When we create a pod yaml and apply it like we have been, WE are solely responsible for the lifecycle of the pod (k8s is not, because we have created it manually). If it dies or gets deleted, it's gone, and so is our app.

A replicaset can save the day, and it's easy - it's just a little bit of config we give to configuration. All we do is tell k8s that we want x number of the specific pods running at ALL times - if one of the pods dies, k8s will immediately create a new pod to replace it. Think of a replicaset as a wrapper over a pod.

To define a replicaset,check the k8s api documentation. We do not need to create a seperate pod and replicaset yaml file. You should write a 2 in 1. The spec.metadata inside a replicaset is the exact same as the top-level metadata you would put in your pod, the difference is just that now (for the pod) you use spec.template to define the metadata.labels, spec.template.spec , etc.

/////////////////////////////////////

Deployments:

A deployment is a more sophisticated form of replicasets. With a deployment, we can get automatic rolling updates with 0 downtime. They also support rollbacks in case something goes wrong.

A deployment will actually create it's own replicaset - the deployment's job is to create and manage a replicaset and all it's objects.

From now on, we won't create replicasets, we will only create deployments.

/////////////////////////////////

kubectl rollout status (command):

kubectl rollout status <object> <objectName>

exp:

kubectl rollout status deploy/some-deployment

You can watch the rollout status while it's happening by issuing that command, it's in real-time. Very convenient.

Undo exp:

To undo a rollout while it's happening

First check the history:

kubectl rollout history <object> <objectName>
exp:
kubectl rollout history deploy/myDeployment

Will give you id's of the previous rollouts that have happened. To roll back to previous:

kubectl rollout undo <object> <objectName>
exp:
kubectl rollout undo deploy/myDeployment

(k8s will by default remember only the last 10 rollouts)

Remember - when you roll back or do things with the command line, pay attention because your yaml files will be out of sync with your current state, and you always want desired state to match current state, so update your yaml when needed. 

Only use rollbacks in complete emergencies because you'll get desired vs. current state out of sync.

/////////////////////////////////////////////

Persistance:

Now that we have deployed all of our microservices, there is a serious problem. All of the history of our vehicles is being stored in memory of the position tracker pod. The problem is that eventually, the pod will run out of memory and k8s will destroy it and restart it. To solve this problem, were going to need to store this data in an actual data store, like a database somewhere.

Our next step is to create a new deployment/pod for a dedicated mongodb data store - it will be used ONLY to store data from the position tracker in a permanent store.

Well need a new mongo deployment and service.

Once we have created the new mongo deployment/pod and service, we still have a problem - it definitely stores the data from the position-tracker, but its only storing it inside that pods memory. When that pod/container dies, although the replicaset will bring it back up, the mongodb data is gone forever. We now need to persist the data in a persistent volume, so that the mongodb data survives a pod restart.

In our case, we want to store the data somewhere on our local machine right now, since we are developing on localhost. When we end up migrating to aws, we will want to utilize an EBS (elastic block store - a hard drive in the cloud).

To mount a volume, we need to tell our deployment that we want to mount a container folder (the mongodb data folder) outside on our localhost to some arbitrary folder.

in the deployment.spec.template.spec.containers, add a 'volumeMounts' property -
  with a -name: mongo-persistent-storage
          mountPath: /data/db #default mongod data folder

For the actual mount path on our localmachine, there are many complicated options. Refer to the k8s documentation and refer to our yaml file to see how we mount to a directory inside our VM for the mongo data store.

example:

deployment.spec.template.spec:
  
  volumes:
    - name: some-name
      hostPath:
        path: /some/mount/
        type: DirectoryOrCreate

The path above must match the path you set in volumeMounts under the containers level. Using directoryOrCreate will create the directory you specify on the local host/VM if not exists.

The problem with the above method is that if you have that hostPath in many different files, it's hardcoded and if you needed to change the path you have to do it in a bunch of places. A better method is to use a 'pointer' which is a pointer to the configuration of HOW we want the mount to be implemented - that way if you update the configuration pointer, all yaml files that use it will be updated. For this we use a persistentVolumeClaim

so under deployment.spec.tempate.spec.volumes:

undrneath the name field, add:

persistentVolumeClaim:
  claimName: file-name-for-claim

Then you would create a new object or yaml file, which the above points to (a file can be named anything):

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: file-name-for-claim #must match the claimName above
spec:
  # Allocate disk space
  accessMode:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi #20 gigabytes of storage

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: any-name
spec:
  accessMode:
    - ReadWriteOnce
  capacity:
    storage: 20Gi
  # Use any type of storage, in our case hostPath
  hostPath:
    path: "/some/mount/path"
    type: DirectoryOrCreate

Notice how our new file is broken up into 2 objects? The first object tells k8s WHAT we want for volume claim, the second object tells k8s HOW we want to implement it (i.e. EBS, hostPath, etc.)

The above is slightly complicated but just reference the documentation and remember that we have 2 seperate needs in our new file - what we need and how we want to implement.

So now that we have both a persistentVolumeClaim, and a persistentVolume, we need to actually link them together to make it work. For this, we need a storageClass. The easiest way to do this is to add a storageClassName to each object.

So in our PersistentVolumeClaim.spec:

storageClassName: someStorageClassName

Also in our PersistentVolume.spec:

storageClassName: someStorageClassName

Make sure the name of the class is the same in both object.

A storage class itself allows admins to set up different storage classes, such as provisioning SSD drives, hard drives, etc. So you could create a configuration for both the SSD and hard drive, and this configuration would label it and define which type it is. This is more important for the cloud but well use it for our hostpath.

We won't create a storage class object, but we will reference a storageClass name in both our PVC and PV, that way they link together. Later on we'll actually create different types of storageClasses for different storage types (ssd, EBS, etc)..... Then we will create new PV's and PVC's that will link together using different storage classes....











